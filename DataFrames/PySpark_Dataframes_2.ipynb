{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pyspark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:11:34.655819400Z",
     "start_time": "2024-01-01T06:11:33.984104700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:11:53.541082900Z",
     "start_time": "2024-01-01T06:11:53.541082900Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('DataFrame2').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:12:29.811382800Z",
     "start_time": "2024-01-01T06:12:16.851599400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x153c31c07f0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-V83NRHM:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>DataFrame2</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:12:40.062008700Z",
     "start_time": "2024-01-01T06:12:38.754317900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameters in Dropping functionalities\n",
    "- Handling Missing values by Mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df = spark.read.csv('test2.csv', header = True, inferSchema = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:28:12.501241400Z",
     "start_time": "2024-01-01T06:28:12.165383100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+\n",
      "|    Name| Age|Experience|   Location|Salary|\n",
      "+--------+----+----------+-----------+------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|\n",
      "|  Sameer|NULL|      NULL|       NULL| 50000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|\n",
      "|    NULL|  25|      NULL|       NULL|  NULL|\n",
      "|    NULL|  22|      NULL|       NULL|  NULL|\n",
      "+--------+----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:28:17.218157400Z",
     "start_time": "2024-01-01T06:28:16.890049400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropping Columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|  Priyam|  23|         2|125000|\n",
      "|  Kartik|  23|         2| 66000|\n",
      "| Kshitiz|  22|         1| 25000|\n",
      "|Akanksha|  24|         2| 60000|\n",
      "|   Akhil|  23|         2| 70000|\n",
      "|   Rohit|NULL|         1|  NULL|\n",
      "|   Anmol|  22|         1| 40000|\n",
      "|  Sameer|NULL|      NULL| 50000|\n",
      "|   Sakib|  22|         1| 25000|\n",
      "| Prateek|  23|         0|     0|\n",
      "|    NULL|  25|      NULL|  NULL|\n",
      "|    NULL|  22|      NULL|  NULL|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.drop('Column Name')\n",
    "\n",
    "df.drop('Location').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:30:01.601726500Z",
     "start_time": "2024-01-01T06:30:01.378570200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropping Null Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+\n",
      "|    Name| Age|Experience|   Location|Salary|\n",
      "+--------+----+----------+-----------+------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|\n",
      "|  Sameer|NULL|      NULL|       NULL| 50000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|\n",
      "|    NULL|  25|      NULL|       NULL|  NULL|\n",
      "|    NULL|  22|      NULL|       NULL|  NULL|\n",
      "+--------+----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:31:14.952402500Z",
     "start_time": "2024-01-01T06:31:14.806378900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-----------+------+\n",
      "|    Name|Age|Experience|   Location|Salary|\n",
      "+--------+---+----------+-----------+------+\n",
      "|  Priyam| 23|         2|  Bangalore|125000|\n",
      "|  Kartik| 23|         2|      Noida| 66000|\n",
      "| Kshitiz| 22|         1|     Mumbai| 25000|\n",
      "|Akanksha| 24|         2|  Bangalore| 60000|\n",
      "|   Akhil| 23|         2|  Bangalore| 70000|\n",
      "|   Anmol| 22|         1|  Hyderabad| 40000|\n",
      "|   Sakib| 22|         1|     Mumbai| 25000|\n",
      "| Prateek| 23|         0|Robertsganj|     0|\n",
      "+--------+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()\n",
    "# IF no parameters, all the rows with null values will be dropped"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:32:26.637408100Z",
     "start_time": "2024-01-01T06:32:26.459128100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### how parameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-----------+------+\n",
      "|    Name|Age|Experience|   Location|Salary|\n",
      "+--------+---+----------+-----------+------+\n",
      "|  Priyam| 23|         2|  Bangalore|125000|\n",
      "|  Kartik| 23|         2|      Noida| 66000|\n",
      "| Kshitiz| 22|         1|     Mumbai| 25000|\n",
      "|Akanksha| 24|         2|  Bangalore| 60000|\n",
      "|   Akhil| 23|         2|  Bangalore| 70000|\n",
      "|   Anmol| 22|         1|  Hyderabad| 40000|\n",
      "|   Sakib| 22|         1|     Mumbai| 25000|\n",
      "| Prateek| 23|         0|Robertsganj|     0|\n",
      "+--------+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'any').show()\n",
    "\n",
    "# Same as drop command with no params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:37:12.778784500Z",
     "start_time": "2024-01-01T06:37:12.581171500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+\n",
      "|    Name| Age|Experience|   Location|Salary|\n",
      "+--------+----+----------+-----------+------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|\n",
      "|  Sameer|NULL|      NULL|       NULL| 50000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|\n",
      "|    NULL|  25|      NULL|       NULL|  NULL|\n",
      "|    NULL|  22|      NULL|       NULL|  NULL|\n",
      "+--------+----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'all').show()\n",
    "\n",
    "# Will only drop the rows with all NULL values only."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:40:34.274914600Z",
     "start_time": "2024-01-01T06:40:34.171606200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### threshold parameter\n",
    "\n",
    "The value of threshold means that at-least that many non null values must be there in the row, if not then the row will be dropped."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+\n",
      "|    Name| Age|Experience|   Location|Salary|\n",
      "+--------+----+----------+-----------+------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|\n",
      "|  Sameer|NULL|      NULL|       NULL| 50000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|\n",
      "+--------+----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'any', thresh = 2).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:47:46.942909600Z",
     "start_time": "2024-01-01T06:47:46.820441Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### subset parameter\n",
    "\n",
    "Only drops rows which have NULL values in the specified column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+\n",
      "|    Name| Age|Experience|   Location|Salary|\n",
      "+--------+----+----------+-----------+------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|\n",
      "+--------+----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = 'any', subset = ['Experience']).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T06:51:05.636162Z",
     "start_time": "2024-01-01T06:51:05.523679400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filling the Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### fill parameters fills a value in the place of NULL values and we can also fill values in a specific column too."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+-------------+------+\n",
      "|         Name| Age|Experience|     Location|Salary|\n",
      "+-------------+----+----------+-------------+------+\n",
      "|       Priyam|  23|         2|    Bangalore|125000|\n",
      "|       Kartik|  23|         2|        Noida| 66000|\n",
      "|      Kshitiz|  22|         1|       Mumbai| 25000|\n",
      "|     Akanksha|  24|         2|    Bangalore| 60000|\n",
      "|        Akhil|  23|         2|    Bangalore| 70000|\n",
      "|        Rohit|NULL|         1|Missing Value|  NULL|\n",
      "|        Anmol|  22|         1|    Hyderabad| 40000|\n",
      "|       Sameer|NULL|      NULL|Missing Value| 50000|\n",
      "|        Sakib|  22|         1|       Mumbai| 25000|\n",
      "|      Prateek|  23|         0|  Robertsganj|     0|\n",
      "|Missing Value|  25|      NULL|Missing Value|  NULL|\n",
      "|Missing Value|  22|      NULL|Missing Value|  NULL|\n",
      "+-------------+----+----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('Missing Value').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:18:13.818800800Z",
     "start_time": "2024-01-01T07:18:13.684352200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Always check data type of columns.\n",
    "\n",
    "You cannot fill string values in the column of numeric data type."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+-----------+------+\n",
      "|    Name|Age|Experience|   Location|Salary|\n",
      "+--------+---+----------+-----------+------+\n",
      "|  Priyam| 23|         2|  Bangalore|125000|\n",
      "|  Kartik| 23|         2|      Noida| 66000|\n",
      "| Kshitiz| 22|         1|     Mumbai| 25000|\n",
      "|Akanksha| 24|         2|  Bangalore| 60000|\n",
      "|   Akhil| 23|         2|  Bangalore| 70000|\n",
      "|   Rohit|  0|         1|       NULL|  NULL|\n",
      "|   Anmol| 22|         1|  Hyderabad| 40000|\n",
      "|  Sameer|  0|      NULL|       NULL| 50000|\n",
      "|   Sakib| 22|         1|     Mumbai| 25000|\n",
      "| Prateek| 23|         0|Robertsganj|     0|\n",
      "|    NULL| 25|      NULL|       NULL|  NULL|\n",
      "|    NULL| 22|      NULL|       NULL|  NULL|\n",
      "+--------+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(00, ['Age']).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:20:43.060903100Z",
     "start_time": "2024-01-01T07:20:42.939681600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling Missing Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "types = []\n",
    "types = df.dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:24:50.534774800Z",
     "start_time": "2024-01-01T07:24:50.501971900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Name', 'string'),\n ('Age', 'int'),\n ('Experience', 'int'),\n ('Location', 'string'),\n ('Salary', 'int')]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:24:40.299871200Z",
     "start_time": "2024-01-01T07:24:40.267207600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Working of Imputer Function\n",
    "\n",
    "\n",
    "1. Importing the Imputer class:\n",
    "\n",
    "    ```from pyspark.ml.feature import Imputer```\n",
    "    This line imports the Imputer class from the pyspark.ml.feature module, which is used for handling missing values in PySpark DataFrames.\n",
    "\n",
    "2. Creating an Imputer object:\n",
    "\n",
    "    ```imputer = Imputer(inputCols=['Age', 'Experience', 'Salary'], outputCols=['Age_imputed', 'Experience_imputed', 'Salary_imputed'])```\n",
    "    This line creates an instance of the Imputer class with the following parameters:\n",
    "    inputCols: A list specifying the names of the columns in the DataFrame that contain missing values. In this case, it's targeting the columns Age, Experience, and Salary.\n",
    "\n",
    "    ```outputCols```: A list specifying the names of the new columns that will be created to hold the imputed values. Here, it creates columns named Age_imputed, Experience_imputed, and Salary_imputed.\n",
    "\n",
    "3. Setting the imputation strategy:\n",
    "\n",
    "    ```.setStrategy('mean')```\n",
    "    This line sets the strategy to use for imputing missing values. The mean strategy specifies that missing values will be filled with the mean (average) of the non-missing values in each column.\n",
    "\n",
    "4. Applying the Imputer:\n",
    "\n",
    "    While not shown in the provided code, the Imputer is typically applied to a DataFrame using a two-step process:\n",
    "    Fitting: ```imputer_model = imputer.fit(df)```\n",
    "    This calculates the necessary statistics (like the mean for each column in this case) based on the provided DataFrame (df).\n",
    "\n",
    "    Transforming: ```df_imputed = imputer_model.transform(df)```\n",
    "    This applies the learned imputation rules to fill in missing values in the DataFrame, creating a new DataFrame (df_imputed) with the imputed columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer (\n",
    "    inputCols = ['Age', 'Experience', 'Salary'],\n",
    "    outputCols = ['{}_new'.format(c) for c in ['Age', 'Experience', 'Salary']]\n",
    ").setStrategy('mean')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:38:49.113657300Z",
     "start_time": "2024-01-01T07:38:49.073479900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding imputated cols to dataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+-------+--------------+----------+\n",
      "|    Name| Age|Experience|   Location|Salary|Age_new|Experience_new|Salary_new|\n",
      "+--------+----+----------+-----------+------+-------+--------------+----------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|     23|             2|    125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|     23|             2|     66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|     22|             1|     25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|     24|             2|     60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|     23|             2|     70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|     22|             1|     51222|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|     22|             1|     40000|\n",
      "|  Sameer|NULL|      NULL|       NULL| 50000|     22|             1|     50000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|     22|             1|     25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|     23|             0|         0|\n",
      "|    NULL|  25|      NULL|       NULL|  NULL|     25|             1|     51222|\n",
      "|    NULL|  22|      NULL|       NULL|  NULL|     22|             1|     51222|\n",
      "+--------+----+----------+-----------+------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df).transform(df).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:38:51.625384Z",
     "start_time": "2024-01-01T07:38:51.225074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+-----------+------+\n",
      "|    Name| Age|Experience|   Location|Salary|\n",
      "+--------+----+----------+-----------+------+\n",
      "|  Priyam|  23|         2|  Bangalore|125000|\n",
      "|  Kartik|  23|         2|      Noida| 66000|\n",
      "| Kshitiz|  22|         1|     Mumbai| 25000|\n",
      "|Akanksha|  24|         2|  Bangalore| 60000|\n",
      "|   Akhil|  23|         2|  Bangalore| 70000|\n",
      "|   Rohit|NULL|         1|       NULL|  NULL|\n",
      "|   Anmol|  22|         1|  Hyderabad| 40000|\n",
      "|  Sameer|NULL|      NULL|       NULL| 50000|\n",
      "|   Sakib|  22|         1|     Mumbai| 25000|\n",
      "| Prateek|  23|         0|Robertsganj|     0|\n",
      "|    NULL|  25|      NULL|       NULL|  NULL|\n",
      "|    NULL|  22|      NULL|       NULL|  NULL|\n",
      "+--------+----+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T07:32:50.032095700Z",
     "start_time": "2024-01-01T07:32:49.950448Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
